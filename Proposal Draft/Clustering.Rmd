---
title: "Clustering"
author: "Jintong Hong"
date: "2024-11-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Clustering 
To address the high dimensionality of our location variable, which contains 81 distinct location categories, we employed a systematic clustering approach based on the India's online economic indicators to create meaningful geographical groupings. The clustering process utilizes key economic metrics **(GDP / household income / or tax revenue)** as features to identify locations with similar economic profiles **(resources)**, and iteratvely assigns data points to distinct clusters based on their similarity.

######
Choose from the three methods:
K-Means: This algorithm partitions the data into a predetermined number of clusters (k). It assigns points to the nearest cluster centroid and then updates the centroids based on the mean of the assigned points.
Hierarchical Clustering: This method builds a tree of clusters. It can be agglomerative (merging clusters) or divisive (splitting clusters).
DBSCAN: This density-based method groups together points that are closely packed together while marking outliers that lie alone in low-density regions.
######

This dimension reduction approach effectively reduce the original 81 categories into a manageable set of economically homogeneous clusters, allowing for more meaningful analysis on the property price while preserving the underlying economic patterns in the housing data.



##########
Data Exploration for Status, Transaction, and Floor

The binary Status variable was encoded with 1 representing "Ready to Move" properties and 0 for all missing values. However, our group decided to exclude the variable from modeling process due to the lack of illustration, and consider the moving status of the property also does not usually affect the housing price by nature. **(potentially we need to do a correlation check between price and status to reassure this point)** The composite Floor variable was decomposed into two distinct numerical variables: Current Floor and Total Floors, with ground floor observations recoded as 0. Due to the presence of significant outliers identified through box plot analysis, both floor variables underwent standardization using StandardScaler. Additionally, to facilitate categorical analysis in case no linear relationship exists for the floor varialbes, Current Floor was further categorized into four ordinal categories using the following thresholds: (-1,5], (5,10], (10,15], and (15,$\infty$). The Transaction variable underwent categorical transformation, with missing values explicitly encoded as NaN, followed by one-hot encoding that generated four binary indicator variables (Transaction_Other, Transaction_Rent/Lease, Transaction_Resale, Transaction_New Property) in boolean form. Correlation analysis was conducted to examine the relationships between the transformed variables, particularly focusing on the associations between floor-related features and transaction types.
##########

## Accuracy test:
Regression Metrics: how to assess the accuracy of prediction
https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics

##########

## Train test split
We would use 80% of the original dataset as the training group for model construction, and the rest 20% as the testing group to assess the model's performance.

##########

## Random Forest

Benefits of using random forest, especially when encounter a dataset comprised of mostly categorical variables.

Generated by ChatGpt:
Using **Random Forest** on a dataset that is mostly made up of categorical variables has several advantages, making it a powerful and effective tool in such scenarios:

### 1. **Handles Categorical Variables Efficiently**  
   - **No need for extensive preprocessing:** Unlike some algorithms that require you to encode categorical variables into numerical formats (e.g., one-hot encoding, label encoding), Random Forest can directly handle categorical variables without extensive preprocessing. Many implementations (like `scikit-learn`'s RandomForestClassifier) can automatically handle categorical inputs.
   - **Ability to split on categorical features:** Random Forest can split on categorical variables effectively by considering different splits and selecting the most informative ones, whether the categorical feature has many or few categories.

### 2. **Robust to Overfitting**  
   - **Ensemble approach:** Random Forest builds multiple decision trees using bootstrapped subsets of the data and random feature selections for each split. This ensemble approach reduces the risk of overfitting, which can be a concern when working with high-dimensional categorical datasets.
   - **Randomness helps generalization:** The random selection of features at each tree split helps avoid memorizing patterns specific to the training data, leading to better generalization on unseen data.

### 3. **Captures Complex Interactions**  
   - **Non-linear relationships:** Random Forest is not limited to linear relationships and can capture complex, non-linear interactions between categorical features. This is especially useful when the relationships in your data are not easily expressed through simple linear models.
   - **Feature interaction modeling:** The decision tree-based structure naturally identifies and models interactions between features, which is often important when dealing with categorical variables that might interact in non-obvious ways.

### 4. **Feature Importance**  
   - **Identify important features:** Random Forest can provide a ranking of features based on their importance, helping you to understand which categorical variables are most influential in predicting the target variable. This can be especially helpful in feature selection and improving model interpretability.
   - **Handles redundant features well:** In categorical datasets, some features may have redundant information or be highly correlated. Random Forest can manage these redundancies more effectively than some other algorithms, as it randomly selects subsets of features at each tree split.

### 5. **Works Well with High-Dimensional Data**  
   - **Scalability with many categories:** Random Forest can handle datasets with a large number of categorical variables and categories within those variables. Even if the dataset is sparse or has many unique categories, the algorithm can still make sense of the data without requiring heavy dimensionality reduction.
   
### 6. **Handles Missing Data**  
   - **Missing value handling:** Random Forest can handle missing values in categorical features by using surrogate splits. These are alternative splits that are used when the primary split feature has missing data, allowing the model to continue learning without needing to impute missing values upfront.
   
### 7. **Versatile and Flexible**  
   - **Works with both classification and regression problems:** Random Forest can be applied to both classification (where the target is categorical) and regression (where the target is continuous) tasks, making it versatile across a wide range of applications.

### 8. **Interpretability with Trees**  
   - **Decision tree insights:** While Random Forest itself is an ensemble model and is generally considered a "black box," individual decision trees within the forest can still provide interpretable insights into how categorical variables influence predictions. Analyzing the individual trees can give you a sense of which categories or combinations of categories are driving model decisions.

### 9. **Resistant to Outliers and Noise**  
   - **Robust to noise:** Because Random Forest builds multiple trees and aggregates predictions, it is less sensitive to noise in the data compared to other models. Outliers or errors in categorical features are unlikely to dominate model predictions due to the averaging process across trees.

### 10. **No Assumption of Data Distribution**  
   - **No parametric assumptions:** Random Forest does not require assumptions about the distribution of the data (such as normality or linearity), which can be a significant advantage when working with categorical data, as categorical variables often do not follow any known distribution patterns.

### In summary:
For datasets with mostly categorical variables, Random Forest offers a highly flexible, robust, and scalable approach. It can handle categorical data efficiently, capture complex interactions, and offer insights through feature importance and decision tree analysis, all while being resistant to overfitting and noise.






